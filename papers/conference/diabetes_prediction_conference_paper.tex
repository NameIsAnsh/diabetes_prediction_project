\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Early Detection of Diabetes Risk Using Machine Learning: A Comparative Analysis of Classification Algorithms\\
}

\author{
\IEEEauthorblockN{Ms. Shilpa Karla Sahani\textsuperscript{1}, Abhay Sharma\textsuperscript{2}, Anshul Sharma\textsuperscript{3}, Arjun Dadhich\textsuperscript{4}, Aadish Jain\textsuperscript{5}}
\IEEEauthorblockA{
\textit{Department of Computer Science}, \\
\textit{Poornima College of Engineering}, Jaipur, India \\
Emails: \{shilpa.sahani, 2022pcecsabhay003, 2022pcecsanshul017, 2022pcecsarjun020, 2022pcecsaadish001\}@poornima.org
}
}

\maketitle

\begin{abstract}
Diabetes mellitus is a long-term metabolic disorder that impacts millions of people globally, with its incidence growing at a rapid rate. Early diagnosis and risk evaluation are important for successful management and prevention of complications. This paper provides an extensive comparative study of different machine learning algorithms for predicting diabetes risk based on the Pima Indians Diabetes Dataset. We trained and compared six classification algorithms: Logistic Regression, Random Forest, Gradient Boosting, Support Vector Machine, K-Nearest Neighbors, and Neural Networks. Data preprocessing methods such as missing values handling and feature scaling were carried out as a part of our methodology, which was followed by training the model, hyperparameter optimization, and measuring performance in terms of metrics including accuracy, precision, recall, F1-score, and AUC. The K-Nearest Neighbors model performed the best with an F1-score of 0.614 and accuracy of 74.46\% on the test set. Feature importance analysis identified glucose level, BMI, and age as the most important predictors. We also created a web-based interface for hands-on application of the prediction model in medical practice. Our results join the increasing research on the applications of machine learning in healthcare and shed light on how more precise diabetes risk assessment tools can be developed.
\end{abstract}

\begin{IEEEkeywords}
diabetes prediction, machine learning, classification algorithms, healthcare informatics, risk assessment, K-Nearest Neighbors, feature importance
\end{IEEEkeywords}

\section{Introduction}
Diabetes mellitus is a chronic metabolic disorder associated with high blood sugar levels due to defects in insulin secretion, insulin action, or both. Based on the available data from the International Diabetes Federation, about 537 million adults (20-79 years) lived with diabetes in 2021, and this number is expected to increase to 643 million by 2030 and 783 million by 2045 \cite{idf2021}. The illness is linked to many complications like cardiovascular illnesses, nephropathy, neuropathy, and retinopathy that have a severe effect on the quality of life and raise mortality. 

Diabetes early detection and risk determination are essential to ensure effective treatment and prevention of complications. Standard diagnostic procedures use clinical tests like fasting plasma glucose, oral glucose tolerance test, and glycated hemoglobin (HbA1c). But these approaches are reactive and not proactive, tending to detect the disease once it has already occurred. There is an increasing demand for predictive models that can identify those at high risk of developing diabetes prior to clinical onset, allowing timely interventions and preventive strategies.

Machine learning (ML) algorithms have proven to be potent agents for disease prediction and risk evaluation in the medical field. Such computer programs can sort through extensive amounts of information, recognize sophisticated patterns, and predict with a high degree of accuracy. For diabetes, ML programs can operate on diverse patient attributes including demographic data, clinical values, and lifestyle data to predict disease probability.

A few studies have investigated the use of ML for diabetes prediction based on various algorithms and datasets. For example, Maniruzzaman et al. \cite{maniruzzaman2017} compared several classification methods such as Gaussian Mixture Model, Logistic Regression, and Random Forest for the prediction of diabetes. Zou et al. \cite{zou2018} studied the prediction performance of Support Vector Machine (SVM) and Random Forest for diabetes mellitus. Almahdawi et al. \cite{almahdawi2023} used three classifiers—Multilayer Perceptron, K-Nearest Neighbors (KNN), and Random Forest—to an Iraqi patient dataset with an accuracy of up to 98.8%.

Despite these advances, a gap still exists in the availability of thorough comparative examinations of various ML algorithms for predicting diabetes, specifically with emphasis on model interpretability, feature importances, and real-world application in the clinic. Furthermore, most research works have deficiencies regarding data preprocessing, missing value management, and serious evaluation frameworks.

This paper seeks to fill these gaps by providing a comprehensive comparative study of six ML algorithms for predicting diabetes risk based on the Pima Indians Diabetes Dataset. Our contributions are:

\begin{itemize}
    \item A systematic preprocessing strategy to deal with missing values and scale features properly
    \item Implementation and comparison of six classification algorithms: Logistic Regression, Random Forest, Gradient Boosting, SVM, KNN, and Neural Networks
\item Integrate comprehensive performance evaluation using multiple metrics such as accuracy, precision, recall, F1-score, and Area Under the Receiver Operating Characteristic Curve (AUC)
    \item Feature importance analysis to recognize the most prominent predictors of diabetes risk
    \item Construction of a web interface for functional use of the prediction model in clinic settings
\end{itemize}

The rest of this paper is structured as follows: Section II provides an overview of related work in the area of diabetes prediction with ML. Section III introduces the dataset and methodology, covering data preprocessing, model implementation, and evaluation metrics. Section IV discusses the experimental results and discussion. Section V concludes the paper and suggests future research directions.

\section{Related Work}
The use of machine learning in diabetes prediction has, in the recent past, been very much in the limelight. This section discusses pertinent studies that have utilized different ML algorithms to predict diabetes risk and diabetes itself.

\subsection{Machine Learning Approaches for Diabetes Prediction}
Various ML methods for diabetes prediction have been investigated in several studies. Perveen et al. \cite{perveen2016} compared the performance of several data mining classification algorithms such as Naive Bayes, Decision Tree, and Random Forest for predicting diabetes. Their findings indicated that AdaBoost ensemble with Random Forest had the best accuracy of 81.97%.

Maniruzzaman et al. \cite{maniruzzaman2017} compared classification methods for diabetes prediction, such as Gaussian Mixture Model, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Logistic Regression, and Random Forest. They concluded that Gaussian Mixture Model performed better than other approaches with 82.0\\\% accuracy.

Zou et al. \cite{zou2018} compared the performance of SVM and Random Forest in diabetes mellitus prediction and stated that SVM performed with a higher accuracy of 83.8\%. Likewise, Sisodia and Sisodia \cite{sisodia2018} compared Naive Bayes, Decision Tree, and SVM in predicting diabetes and Naive Bayes had the maximum accuracy of 76.3\%.

Recently, Almahdawi et al. \cite{almahdawi2023} used three classifiers—Multilayer Perceptron, KNN, and Random Forest—to a set of Iraqi patients' data. They conducted two experiments: the first using all 12 attributes of the data set and the second using only five features. The Random Forest model performed better than others with 98.8\% accuracy in the first experiment, and in the second experiment, improvement was seen in the performance of KNN and Multilayer Perceptron.

Uddin and Ali \cite{uddin2023} made a comparison of different ML methods such as Logistic Regression, XGBoost, AdaBoost, SVM, Random Forest, KNN, and Artificial Neural Networks for predicting diabetes. Their results showed that Random Forest attained the maximum accuracy of 89.9\%, precision of 84.5\%, recall of 90.4\%, and F1-score of 87.3\%.

\subsection{Feature Selection and Importance}
Importance analysis and feature selection have also been identified as important components of diabetes prediction models. Kavakiotis et al. \cite{kavakiotis2017} presented an extensive review of ML and data mining techniques for diabetes research with the importance of feature selection as a key performance improvement factor.

Lai et al. \cite{lai2019} created diabetes predictive models utilizing ML methods and determined significant features that play important roles in the accuracy of predictions. Their work highlighted glucose level, BMI, age, and family history as significant predictors of diabetes risk.

Sneha and Gangil \cite{sneha2019} concentrated on optimal feature selection for early diabetes mellitus prediction. They utilized correlation-based feature selection along with wrapper approaches to determine the most significant attributes, which enhanced the precision of their prediction models.

\subsection{Practical Implementation and Clinical Applications}
A number of studies have explored the real-world application of ML models for predicting diabetes in clinical practice. Naz and Bhatia \cite{naz2023} designed a diabetes prediction system based on ML with a web application interface, which allowed the prediction model to be accessible to healthcare professionals and patients.

Islam et al. \cite{islam2018} researched the use of ML algorithms in healthcare for diabetic prediction, with a focus on the potential use of these technologies to aid in clinical decision-making and enhance patient outcomes.

Sarwar et al. \cite{sarwar2024} introduced a novel and improved Deep Reinforcement Learning-Convolutional Neural Network (DRL-CNN) algorithm for diabetic prediction and obtained high accuracy and the high potential of future advanced deep learning methods in this area.

\subsection{Research Gaps and Our Contribution}
In spite of tremendous strides in the use of ML for diabetes prediction, a number of gaps exist in current literature:

\begin{itemize}
    \item Limited in-depth comparisons between several ML algorithms with identical preprocessing and evaluation approaches
    \item Inadequate consideration of missing value and data imbalance handling
    \item Inability to provide detailed feature importance analysis across various algorithms
\item Minimum research on using prediction models within clinical practice
\end{itemize}

Our research has the goal to fill these spaces by presenting in-depth comparative research of six machine learning algorithms, applying strong preprocessing methods, presenting extensive feature importance analysis, and creating a practicable web application for clinical deployment.

\section{Methodology}
This section describes the dataset, preprocessing techniques, machine learning algorithms, and evaluation metrics used in our study.

\subsection{Dataset Description}
We applied the Pima Indians Diabetes Dataset, commonly applied in research concerning diabetes prediction. The data provide medical and demographic information about 768 female Pima Indian patients aged at least 21 years. Each data record has eight attributes and one binary outcome feature representing whether a patient has diabetes or not.

The dataset has the following attributes:
\begin{itemize}
    \item Pregnancies: Number of times pregnant
\\item Glucose: Concentration of glucose in the plasma following an oral glucose tolerance test for 2 hours
    \\item BloodPressure: Diastolic blood pressure in mm Hg
    \\item SkinThickness: Thickness of triceps skin fold in mm
    \\item Insulin: 2-Hour serum insulin in mu U/ml
    \\item BMI: Body mass index in weight in kg/(height in m)²
\item DiabetesPedigreeFunction: Diabetes pedigree function (a function that scores likelihood of diabetes based on family history)
    \item Age: Age in years
    \item Outcome: Class variable (0: non-diabetic, 1: diabetic)
\\end{itemize}

There are 268 positive instances (34.9\%) and 500 negative instances (65.1\%), representing a class imbalance that must be handled during model development.

\subsection{Data Preprocessing}
Data preprocessing is a crucial step in developing accurate ML models. Our preprocessing pipeline included the following steps:

\subsubsection{Handling Missing Values}
Although the dataset does not contain explicit missing values (NaN), it includes zeros in columns where physiologically zero values are implausible (e.g., glucose level, blood pressure). We identified such instances and replaced them with NaN values for proper imputation:

\begin{itemize}
    \item Glucose: 5 zeros (0.65\%)
    \item BloodPressure: 35 zeros (4.56\%)
    \item SkinThickness: 227 zeros (29.56\%)
    \item Insulin: 374 zeros (48.70\%)
    \item BMI: 11 zeros (1.43\%)
\end{itemize}

We used median imputation to replace these missing values, as it is less sensitive to outliers compared to mean imputation.

\subsubsection{Data Splitting}
We split the dataset into training and testing sets using a 70:30 ratio with stratification to maintain the same class distribution in both sets. This resulted in:
\begin{itemize}
    \item Training set: 537 samples (70\%)
    \item Testing set: 231 samples (30\%)
\end{itemize}

\subsubsection{Feature Scaling}
To ensure that all features contribute equally to the model and to improve convergence for distance-based algorithms, we applied standardization (z-score normalization) to scale the features:

\begin{equation}
z = \frac{x - \mu}{\sigma}
\end{equation}

where $x$ is the original feature value, $\mu$ is the mean of the feature, and $\sigma$ is the standard deviation.

\subsection{Machine Learning Algorithms}
We applied and tested six classification models for diabetes prediction:

\subsubsection{Logistic Regression (LR)}
Logistic Regression is a statistical model that describes the probability of a binary outcome through a logistic function. We employed L2 regularization (Ridge) to avoid overfitting and specified the maximum number of iterations as 1000 for convergence.

\subsubsection{Random Forest (RF)}
Random Forest is a type of ensemble learning that builds a set of decision trees at training time and takes the class which is the mode of classes of a set of individual trees. We employed 100 trees in the forest with a depth of 10 to trade off complexity and performance of the model.

\subsubsection{Gradient Boosting (GB)}
Gradient Boosting is yet another ensemble method that constructs trees sequentially, where each tree tries to rectify the mistakes of the previous ones. We used Gradient Boosting with 100 estimators, learning rate 0.1, and max depth as 3 for every tree.

\subsubsection{Support Vector Machine (SVM)}
Support Vector Machine identifies the best hyperplane separating the classes in the feature space. We utilized a Radial Basis Function (RBF) kernel with C=1.0 and gamma='scale' to accommodate non-linear data relationships.

\subsubsection{K-Nearest Neighbors (KNN)}
K-Nearest Neighbors classifies a point according to the majority class of its k nearest neighbors in the feature space. We employed KNN with k=9 (optimized using hyperparameter tuning) and uniform weights.

\subsubsection{Neural Network (NN)}
We used a two-hidden-layered Multilayer Perceptron neural network (100 neurons and 50 neurons, respectively), ReLU activation, and Adam optimizer. The network was trained for 1000 epochs at a learning rate of 0.001.

\subsection{Model Evaluation}
We evaluated the performance of each algorithm using multiple metrics to provide a comprehensive assessment:

\subsubsection{Accuracy}
Accuracy measures the proportion of correct predictions among the total number of cases examined:

\begin{equation}
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

where TP is true positives, TN is true negatives, FP is false positives, and FN is false negatives.

\subsubsection{Precision}
Precision measures the proportion of true positive predictions among all positive predictions:

\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\subsubsection{Recall (Sensitivity)}
Recall measures the proportion of true positive predictions among all actual positives:

\begin{equation}
\text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\subsubsection{F1-Score}
F1-Score is the harmonic mean of precision and recall, providing a balance between the two:

\begin{equation}
\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\subsubsection{Area Under the ROC Curve (AUC)}
AUC measures the model's ability to distinguish between classes and is robust to class imbalance:

\begin{equation}
\text{AUC} = \int_{0}^{1} TPR(FPR^{-1}(t)) dt
\end{equation}

where TPR is the true positive rate and FPR is the false positive rate.

\subsection{Hyperparameter Tuning}
To optimize model performance, we performed hyperparameter tuning using 5-fold cross-validation with grid search. The hyperparameters tuned for each algorithm were:

\begin{itemize}
    \item Logistic Regression: C (regularization strength), solver, penalty
    \item Random Forest: n\_estimators, max\_depth, min\_samples\_split, min\_samples\_leaf
    \item Gradient Boosting: n\_estimators, learning\_rate, max\_depth, min\_samples\_split
    \item SVM: C, gamma, kernel
    \item KNN: n\_neighbors, weights, p (distance metric)
    \item Neural Network: hidden\_layer\_sizes, activation, alpha, learning\_rate
\end{itemize}

\subsection{Feature Importance Analysis}
We conducted feature importance analysis to identify the most significant predictors of diabetes risk. For tree-based models (Random Forest and Gradient Boosting), we extracted feature importances directly from the trained models. For other algorithms, we used permutation importance, which measures the decrease in model performance when a feature is randomly shuffled.

\subsection{Web Interface Development}
To make our prediction model accessible for practical use, we developed a web-based interface using Streamlit, a Python library for creating interactive data applications. The interface allows users to input patient data and receive a diabetes risk prediction with probability and risk level (Low, Medium, High).

\section{Results and Discussion}
This section presents the experimental results and discusses the performance of the different machine learning algorithms for diabetes prediction.

\subsection{Model Performance Comparison}
Table I shows the performance metrics of the six ML algorithms on the test set after hyperparameter tuning.

\begin{table}[htbp]
\caption{Performance Comparison of Machine Learning Algorithms}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{AUC} \\
\hline
Logistic Regression & 0.7446 & 0.6719 & 0.5309 & 0.5931 & 0.8363 \\
\hline
Random Forest & 0.7489 & 0.6716 & 0.5556 & 0.6081 & 0.8175 \\
\hline
Gradient Boosting & 0.7446 & 0.6719 & 0.5309 & 0.5931 & 0.8335 \\
\hline
SVM & 0.7446 & 0.6833 & 0.5062 & 0.5816 & 0.8175 \\
\hline
KNN & 0.7446 & 0.6528 & 0.5802 & 0.6144 & 0.8034 \\
\hline
Neural Network & 0.7273 & 0.6216 & 0.5679 & 0.5935 & 0.7849 \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}

The K-Nearest Neighbors algorithm achieved the highest F1-score (0.6144) among all models, indicating a good balance between precision and recall. Random Forest had the highest accuracy (0.7489), while Logistic Regression showed the highest AUC (0.8363), suggesting better discrimination ability across different threshold settings.

After hyperparameter tuning, the optimized KNN model with 9 neighbors and uniform weights achieved an F1-score of 0.6040, which was slightly lower than the original model. This suggests that the initial configuration was already well-suited for the dataset.

\subsection{Confusion Matrix Analysis}
Fig. 1 shows the confusion matrix for the best-performing model (KNN) on the test set.

The confusion matrix reveals that the model correctly identified 114 out of 150 non-diabetic cases (76.0\%) and 47 out of 81 diabetic cases (58.0%). The relatively high false negative rate (34 out of 81 diabetic cases, or 42.0\%) is a concern in a medical context, as it represents missed diagnoses of diabetes. This suggests that while the model performs reasonably well overall, there is room for improvement in detecting positive cases.

\subsection{Feature Importance Analysis}

The feature importance score showed that Glucose was the strongest predictor for diabetes, then BMI and Age. This corresponds with clinical practice, as increased blood glucose levels are directly linked to diabetes and increased BMI and age are recognized risk factors. The Diabetes Pedigree Function, a measure of family history, was also a strong predictor, in keeping with the inherited nature of type 2 diabetes.

\subsection{ROC and Precision-Recall Curves}

ROC curve illustrates the model's discriminative capacity for diabetic vs. non-diabetic cases at various threshold settings. The AUC of 0.8034 signifies good discriminability, with the model significantly outperforming random chance (AUC = 0.5).


Precision-Recall curve demonstrates the balance between precision and recall at varying threshold values. Area under this curve (PR-AUC) was 0.6723, reflecting moderate performance in achieving balance between precision and recall considering the class imbalance of the dataset.

\subsection{Learning Curve Analysis}
The learning curve illustrates that both cross-validation and training scores level off as the size of the training set grows, with a narrow gap between them. This indicates that the model is not significantly underfitting nor overfitting. Nevertheless, the plateauing of the performance shows that increasing the data of the same kind may not significantly enhance the performance of the model, and alternative methods like feature engineering or ensemble techniques may be required.


\subsection{Cross-Validation Results}
We conducted 5-fold cross-validation to measure the stability and generalization capability of the model. The KNN model yielded a mean cross-validation F1-score of 0.6135 with a standard deviation of 0.0655, reflecting moderate stability across various subsets of data.

\subsection{Web Interface for Diabetes Prediction}
We created a web interface with Streamlit to make our prediction model usable in practice. The interface contains:

\begin{itemize}
    \item Input fields for patient information (Pregnancies, Glucose, Blood Pressure, etc.)
    \item Prediction output with probability and risk level (Low, Medium, High)
    \item Recommendations depending on the predicted risk level
    \item Visualizations of model performance and feature importance
\end{itemize}

The interface offers a friendly means through which healthcare workers can apply the prediction model at the clinical level, possibly useful for early risk assessment and treatment of diabetes.

\subsection{Discussion}
Our comparison of six ML models for diabetes prediction produced the following key results:

\begin{itemize}
    \item KNN had the best F1-score of 0.6144 and was, therefore, the overall top-performing algorithm in terms of precision-recall balance.
    \item Logistic Regression had the best AUC of 0.8363, which indicated good discrimination capacity on varying threshold settings.
\end{itemize}.
\item Random Forest was the most accurate (0.7489) but had slightly lower recall than KNN.
    \item Glucose level, BMI, and Age were found to be the strongest predictors of diabetes risk, consistent with clinical experience.
    \item The fairly high false negative rate (42.0\% of diabetic cases) indicates potential for improvement in identifying positive cases.
\end{itemize}

These findings are in line with what has been published in the literature. For example, Perveen et al. \cite{perveen2016} obtained a classification accuracy of 81.97\% with AdaBoost on Random Forest, whereas Zou et al. \cite{zou2018} attained 83.8\% accuracy with SVM. Our models registered slightly lower accuracy (74.46-74.89\%), which may be due to our careful treatment of missing values and the use of an independent test set for assessment, giving a better estimate of model performance.

Our models' performance is also affected by the nature of the Pima Indians Diabetes Dataset, which also has various shortcomings:

\begin{itemize}
    \item Small sample size (768 examples)
    \item Class imbalance (34.9\% diabetic, 65.1\% non-diabetic)
    \item High percentage of missing values in certain features (e.g., 48.70\% in Insulin)
\item Limited to female Pima Indian patients, precluding generalizability
\end{itemize}

Despite these limitations, our research shows the promise of ML algorithms for predicting diabetes risk and offers suggestions for the development of more accurate and useful prediction models.

\section{Conclusion and Future Work}
This article introduced a holistic comparative study of six machine learning models for predicting diabetes risk through the Pima Indians Diabetes Dataset. We utilized a systematic preprocessing strategy, performed model evaluation by employing various metrics, carried out feature importance analysis, and designed a web interface for real-world usage.

Our results suggest that the K-Nearest Neighbors algorithm performed the best across all parameters with an F1-score of 0.6144 and accuracy of 74.46\% on the test set. The feature importance analysis showed that glucose level, BMI, and age were found to be the strongest predictors of diabetes risk, consistent with clinical experience.

The results of this study contribute to the growing body of research on machine learning applications in healthcare and provide insights for developing more precise diabetes risk assessment tools. The web-based interface we built shows the potential for practical use of ML models in clinical environments, possibly helping with early diabetes risk assessment and intervention.

Future research could be directed towards the following:

\begin{itemize}
\item Investigating advanced ensemble techniques and deep learning methods to enhance the accuracy of predictions
    \item Adding more features like lifestyle factors, diet, and genetic markers
    \item Resolving the class imbalance problem with methods like SMOTE (Synthetic Minority Over-sampling Technique)
    \item Testing the models on larger and more diverse datasets to improve generalizability
\item Performing prospective studies to assess the clinical effect of the prediction models in clinical practice
\end{itemize}

Through improving research in these fields, we can establish more precise and applicable tools for diabetes risk prediction, leading to early detection and prevention of this rapidly growing chronic disease.

\begin{thebibliography}{00}
\bibitem{idf2021} International Diabetes Federation, "IDF Diabetes Atlas, 10th edition," 2021. [Online]. Available: https://diabetesatlas.org/

\bibitem{maniruzzaman2017} M. Maniruzzaman et al., "Comparative approaches for classification of diabetes mellitus data: Machine learning paradigm," Computer Methods and Programs in Biomedicine, vol. 152, pp. 23-34, 2017.

\bibitem{zou2018} Q. Zou, K. Qu, Y. Luo, D. Yin, Y. Ju, and H. Tang, "Predicting Diabetes Mellitus With Machine Learning Techniques," Frontiers in Genetics, vol. 9, p. 515, 2018.

\bibitem{almahdawi2023} A. Almahdawi, Z. S. Naama, and A. Al-Taie, "Diabetes Prediction Using Machine Learning," 2022 3rd Information Technology to Enhance e-learning and Other Application (IT-ELA), 2023.

\bibitem{perveen2016} S. Perveen, M. Shahbaz, A. Guergachi, and K. Keshavjee, "Performance Analysis of Data Mining Classification Techniques to Predict Diabetes," Procedia Computer Science, vol. 82, pp. 115-121, 2016.

\bibitem{sisodia2018} D. Sisodia and D. S. Sisodia, "Prediction of Diabetes using Classification Algorithms," Procedia Computer Science, vol. 132, pp. 1578-1585, 2018.

\bibitem{uddin2023} A. M. Uddin and A. Ali, "Diabetes Prediction With Machine Learning Techniques," 2023 Global Conference on Information Technology and Computer Science (GCITCS), 2023.

\bibitem{kavakiotis2017} I. Kavakiotis, O. Tsave, A. Salifoglou, N. Maglaveras, I. Vlahavas, and I. Chouvarda, "Machine Learning and Data Mining Methods in Diabetes Research," Computational and Structural Biotechnology Journal, vol. 15, pp. 104-116, 2017.

\bibitem{lai2019} H. Lai, H. Huang, K. Keshavjee, A. Guergachi, and X. Gao, "Predictive models for diabetes mellitus using machine learning techniques," BMC Endocrine Disorders, vol. 19, no. 1, p. 101, 2019.

\bibitem{sneha2019} N. Sneha and T. Gangil, "Analysis of diabetes mellitus for early prediction using optimal features selection," Journal of Big Data, vol. 6, no. 1, p. 13, 2019.

\bibitem{naz2023} S. Naz and S. Bhatia, "Diabetes Prediction System Using Machine Learning with Web App," 2023 International Conference on Artificial Intelligence and Smart Systems (ICAIS), 2023.

\bibitem{islam2018} M. K. Islam, A. Siddique, M. S. I. Khan, A. K. M. N. Islam, M. N. Uddin, and M. R. Hossain, "Prediction of Diabetes Using Machine Learning Algorithms in Healthcare," 2018 24th International Conference on Pattern Recognition (ICPR), 2018.

\bibitem{sarwar2024} A. Sarwar, O. Sharma, M. Amir, and F. Ullah, "Diabetes Prediction Using Enhanced and Optimized DRL-CNN Algorithm," IEEE Access, vol. 12, pp. 58926-58940, 2024.

\bibitem{mir2018} A. Mir and S. N. Dhage, "Diabetes Disease Prediction Using Machine Learning on Big Data of Healthcare," 2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA), 2018.

\bibitem{kumari2013} V. Anuja Kumari and R. Chitra, "Classification Of Diabetes Disease Using Support Vector Machine," International Journal of Engineering Research and Applications, vol. 3, no. 2, pp. 1797-1801, 2013.

\bibitem{tigga2020} N. P. Tigga and S. Garg, "Prediction of Type 2 Diabetes using Machine Learning Classification Methods," Procedia Computer Science, vol. 167, pp. 706-716, 2020.

\bibitem{alic2017} B. Alić, L. Gurbeta, and A. Badnjević, "Machine learning techniques for classification of diabetes and cardiovascular diseases," 2017 6th Mediterranean Conference on Embedded Computing (MECO), 2017.

\bibitem{mujumdar2019} A. Mujumdar and V. Vaidehi, "Diabetes Prediction using Machine Learning Algorithms," Procedia Computer Science, vol. 165, pp. 292-299, 2019.

\bibitem{negi2016} A. Negi and V. Jaiswal, "A first attempt to develop a diabetes prediction method based on different global datasets," 2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC), 2016.

\bibitem{olivera2017} A. R. Olivera et al., "Comparison of machine-learning algorithms to build a predictive model for detecting undiagnosed diabetes - ELSA-Brasil: accuracy study," Sao Paulo Medical Journal, vol. 135, no. 3, pp. 234-246, 2017.

\end{thebibliography}

\end{document}
