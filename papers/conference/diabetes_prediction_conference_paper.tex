\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Early Detection of Diabetes Risk Using Machine Learning: A Comparative Analysis of Classification Algorithms\\
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Author Name}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University Name}\\
City, Country \\
email@domain.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Author Name}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University Name}\\
City, Country \\
email@domain.com}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Author Name}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University Name}\\
City, Country \\
email@domain.com}
}

\maketitle

\begin{abstract}
Diabetes mellitus is a chronic metabolic disorder affecting millions worldwide, with its prevalence increasing at an alarming rate. Early detection and risk assessment are crucial for effective management and prevention of complications. This paper presents a comprehensive comparative analysis of various machine learning algorithms for diabetes risk prediction using the Pima Indians Diabetes Dataset. We implemented and evaluated six classification algorithms: Logistic Regression, Random Forest, Gradient Boosting, Support Vector Machine, K-Nearest Neighbors, and Neural Networks. Our methodology included data preprocessing techniques to handle missing values and feature scaling, followed by model training, hyperparameter tuning, and performance evaluation using metrics such as accuracy, precision, recall, F1-score, and AUC. The K-Nearest Neighbors algorithm achieved the best performance with an F1-score of 0.614 and accuracy of 74.46\% on the test set. Feature importance analysis revealed that glucose level, BMI, and age were the most significant predictors. Additionally, we developed a web-based interface for practical application of the prediction model in clinical settings. Our findings contribute to the growing body of research on machine learning applications in healthcare and provide insights for developing more accurate diabetes risk assessment tools.
\end{abstract}

\begin{IEEEkeywords}
diabetes prediction, machine learning, classification algorithms, healthcare informatics, risk assessment, K-Nearest Neighbors, feature importance
\end{IEEEkeywords}

\section{Introduction}
Diabetes mellitus is a chronic metabolic disorder characterized by elevated blood glucose levels resulting from defects in insulin secretion, insulin action, or both. According to the International Diabetes Federation, approximately 537 million adults (20-79 years) were living with diabetes in 2021, and this number is projected to rise to 643 million by 2030 and 783 million by 2045 \cite{idf2021}. The disease is associated with numerous complications including cardiovascular diseases, nephropathy, neuropathy, and retinopathy, which significantly impact quality of life and increase mortality risk.

Early detection and risk assessment of diabetes are crucial for effective management and prevention of complications. Traditional diagnostic methods rely on clinical tests such as fasting plasma glucose, oral glucose tolerance test, and glycated hemoglobin (HbA1c). However, these methods are reactive rather than proactive, often detecting the disease after it has already developed. There is a growing need for predictive models that can identify individuals at high risk of developing diabetes before clinical manifestation, enabling timely interventions and preventive measures.

Machine learning (ML) techniques have emerged as powerful tools for disease prediction and risk assessment in healthcare. These computational methods can analyze large volumes of data, identify complex patterns, and make predictions with high accuracy. In the context of diabetes, ML algorithms can process various patient attributes such as demographic information, clinical measurements, and lifestyle factors to predict the likelihood of developing the disease.

Several studies have explored the application of ML for diabetes prediction using different algorithms and datasets. For instance, Maniruzzaman et al. \cite{maniruzzaman2017} compared various classification techniques including Gaussian Mixture Model, Logistic Regression, and Random Forest for diabetes prediction. Zou et al. \cite{zou2018} investigated the performance of Support Vector Machine (SVM) and Random Forest in predicting diabetes mellitus. Almahdawi et al. \cite{almahdawi2023} applied three classifiers—Multilayer Perceptron, K-Nearest Neighbors (KNN), and Random Forest—to a dataset of Iraqi patients, achieving up to 98.8\% accuracy.

Despite these advancements, there remains a need for comprehensive comparative analyses of different ML algorithms for diabetes prediction, particularly focusing on model interpretability, feature importance, and practical implementation in clinical settings. Additionally, most existing studies have limitations in terms of data preprocessing, handling of missing values, and rigorous evaluation methodologies.

This paper aims to address these gaps by presenting a thorough comparative analysis of six ML algorithms for diabetes risk prediction using the Pima Indians Diabetes Dataset. Our contributions include:

\begin{itemize}
    \item A systematic preprocessing approach to handle missing values and scale features appropriately
    \item Implementation and evaluation of six classification algorithms: Logistic Regression, Random Forest, Gradient Boosting, SVM, KNN, and Neural Networks
    \item Comprehensive performance assessment using multiple metrics including accuracy, precision, recall, F1-score, and Area Under the Receiver Operating Characteristic Curve (AUC)
    \item Feature importance analysis to identify the most significant predictors of diabetes risk
    \item Development of a web-based interface for practical application of the prediction model in clinical settings
\end{itemize}

The remainder of this paper is organized as follows: Section II reviews related work in the field of diabetes prediction using ML. Section III describes the dataset and methodology, including data preprocessing, model implementation, and evaluation metrics. Section IV presents the experimental results and discussion. Section V concludes the paper and outlines directions for future research.

\section{Related Work}
The application of machine learning for diabetes prediction has gained significant attention in recent years. This section reviews relevant studies that have employed various ML algorithms for diabetes risk assessment and prediction.

\subsection{Machine Learning Approaches for Diabetes Prediction}
Numerous studies have explored different ML approaches for diabetes prediction. Perveen et al. \cite{perveen2016} evaluated the performance of various data mining classification techniques including Naive Bayes, Decision Tree, and Random Forest for diabetes prediction. Their results showed that AdaBoost ensemble with Random Forest achieved the highest accuracy of 81.97\%.

Maniruzzaman et al. \cite{maniruzzaman2017} conducted a comparative study of classification techniques for diabetes prediction, including Gaussian Mixture Model, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Logistic Regression, and Random Forest. They found that Gaussian Mixture Model outperformed other methods with an accuracy of 82.0\%.

Zou et al. \cite{zou2018} investigated the performance of SVM and Random Forest in predicting diabetes mellitus, reporting that SVM achieved better results with an accuracy of 83.8\%. Similarly, Sisodia and Sisodia \cite{sisodia2018} compared Naive Bayes, Decision Tree, and SVM for diabetes prediction, with Naive Bayes showing the highest accuracy of 76.3\%.

More recently, Almahdawi et al. \cite{almahdawi2023} applied three classifiers—Multilayer Perceptron, KNN, and Random Forest—to a dataset of Iraqi patients. Their study involved two experiments: one using all 12 features of the dataset and another using only five attributes. The Random Forest algorithm outperformed others with 98.8\% accuracy in the first experiment, while the second experiment showed improvement in the performance of KNN and Multilayer Perceptron.

Uddin and Ali \cite{uddin2023} compared various ML techniques including Logistic Regression, XGBoost, AdaBoost, SVM, Random Forest, KNN, and Artificial Neural Networks for diabetes prediction. Their findings demonstrated that Random Forest achieved the highest accuracy of 89.9\%, precision of 84.5\%, recall of 90.4\%, and F1-score of 87.3\%.

\subsection{Feature Selection and Importance}
Feature selection and importance analysis have been recognized as crucial aspects of diabetes prediction models. Kavakiotis et al. \cite{kavakiotis2017} conducted a comprehensive review of ML and data mining approaches in diabetes research, highlighting the significance of feature selection in improving model performance.

Lai et al. \cite{lai2019} developed predictive models for diabetes using ML techniques and identified key features that contribute significantly to prediction accuracy. Their study emphasized the importance of glucose level, BMI, age, and family history as strong predictors of diabetes risk.

Sneha and Gangil \cite{sneha2019} focused on optimal feature selection for early prediction of diabetes mellitus. They employed correlation-based feature selection and wrapper methods to identify the most relevant attributes, which improved the accuracy of their prediction models.

\subsection{Practical Implementation and Clinical Applications}
Several studies have addressed the practical implementation of ML models for diabetes prediction in clinical settings. Naz and Bhatia \cite{naz2023} developed a diabetes prediction system using ML with a web application interface, making the prediction model accessible to healthcare providers and patients.

Islam et al. \cite{islam2018} explored the application of ML algorithms in healthcare for diabetes prediction, emphasizing the potential of these technologies to support clinical decision-making and improve patient outcomes.

Sarwar et al. \cite{sarwar2024} proposed an enhanced and optimized Deep Reinforcement Learning-Convolutional Neural Network (DRL-CNN) algorithm for diabetes prediction, achieving high accuracy and demonstrating the potential of advanced deep learning techniques in this domain.

\subsection{Research Gaps and Our Contribution}
Despite the significant progress in applying ML for diabetes prediction, several gaps remain in the existing literature:

\begin{itemize}
    \item Limited comprehensive comparisons of multiple ML algorithms using consistent preprocessing and evaluation methodologies
    \item Insufficient attention to handling missing values and data imbalance issues
    \item Lack of detailed feature importance analysis across different algorithms
    \item Few studies addressing the practical implementation of prediction models in clinical settings
\end{itemize}

Our work aims to address these gaps by providing a thorough comparative analysis of six ML algorithms, implementing robust preprocessing techniques, conducting detailed feature importance analysis, and developing a practical web-based interface for clinical application.

\section{Methodology}
This section describes the dataset, preprocessing techniques, machine learning algorithms, and evaluation metrics used in our study.

\subsection{Dataset Description}
We used the Pima Indians Diabetes Dataset, which is widely used for diabetes prediction research. The dataset contains medical and demographic data of 768 female patients of Pima Indian heritage, aged 21 years and older. Each record includes eight features and a binary outcome variable indicating the presence or absence of diabetes.

The features in the dataset are:
\begin{itemize}
    \item Pregnancies: Number of times pregnant
    \item Glucose: Plasma glucose concentration after a 2-hour oral glucose tolerance test
    \item BloodPressure: Diastolic blood pressure (mm Hg)
    \item SkinThickness: Triceps skin fold thickness (mm)
    \item Insulin: 2-Hour serum insulin (mu U/ml)
    \item BMI: Body mass index (weight in kg/(height in m)²)
    \item DiabetesPedigreeFunction: Diabetes pedigree function (a function that scores likelihood of diabetes based on family history)
    \item Age: Age in years
    \item Outcome: Class variable (0: non-diabetic, 1: diabetic)
\end{itemize}

The dataset has 268 positive cases (34.9\%) and 500 negative cases (65.1\%), indicating a class imbalance that needs to be addressed during model development.

\subsection{Data Preprocessing}
Data preprocessing is a crucial step in developing accurate ML models. Our preprocessing pipeline included the following steps:

\subsubsection{Handling Missing Values}
Although the dataset does not contain explicit missing values (NaN), it includes zeros in columns where physiologically zero values are implausible (e.g., glucose level, blood pressure). We identified such instances and replaced them with NaN values for proper imputation:

\begin{itemize}
    \item Glucose: 5 zeros (0.65\%)
    \item BloodPressure: 35 zeros (4.56\%)
    \item SkinThickness: 227 zeros (29.56\%)
    \item Insulin: 374 zeros (48.70\%)
    \item BMI: 11 zeros (1.43\%)
\end{itemize}

We used median imputation to replace these missing values, as it is less sensitive to outliers compared to mean imputation.

\subsubsection{Data Splitting}
We split the dataset into training and testing sets using a 70:30 ratio with stratification to maintain the same class distribution in both sets. This resulted in:
\begin{itemize}
    \item Training set: 537 samples (70\%)
    \item Testing set: 231 samples (30\%)
\end{itemize}

\subsubsection{Feature Scaling}
To ensure that all features contribute equally to the model and to improve convergence for distance-based algorithms, we applied standardization (z-score normalization) to scale the features:

\begin{equation}
z = \frac{x - \mu}{\sigma}
\end{equation}

where $x$ is the original feature value, $\mu$ is the mean of the feature, and $\sigma$ is the standard deviation.

\subsection{Machine Learning Algorithms}
We implemented and evaluated six classification algorithms for diabetes prediction:

\subsubsection{Logistic Regression (LR)}
Logistic Regression is a statistical method that models the probability of a binary outcome using a logistic function. We used L2 regularization (Ridge) to prevent overfitting and set the maximum number of iterations to 1000 for convergence.

\subsubsection{Random Forest (RF)}
Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the class that is the mode of the classes of individual trees. We used 100 trees in the forest with a maximum depth of 10 to balance model complexity and performance.

\subsubsection{Gradient Boosting (GB)}
Gradient Boosting is another ensemble technique that builds trees sequentially, with each tree correcting the errors of its predecessors. We implemented Gradient Boosting with 100 estimators, a learning rate of 0.1, and a maximum depth of 3 for each tree.

\subsubsection{Support Vector Machine (SVM)}
Support Vector Machine finds the hyperplane that best separates the classes in the feature space. We used a Radial Basis Function (RBF) kernel with C=1.0 and gamma='scale' to handle non-linear relationships in the data.

\subsubsection{K-Nearest Neighbors (KNN)}
K-Nearest Neighbors classifies a data point based on the majority class of its k nearest neighbors in the feature space. We implemented KNN with k=9 (determined through hyperparameter tuning) and uniform weights.

\subsubsection{Neural Network (NN)}
We implemented a Multilayer Perceptron neural network with two hidden layers (100 and 50 neurons, respectively), ReLU activation function, and Adam optimizer. The network was trained for 1000 epochs with a learning rate of 0.001.

\subsection{Model Evaluation}
We evaluated the performance of each algorithm using multiple metrics to provide a comprehensive assessment:

\subsubsection{Accuracy}
Accuracy measures the proportion of correct predictions among the total number of cases examined:

\begin{equation}
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

where TP is true positives, TN is true negatives, FP is false positives, and FN is false negatives.

\subsubsection{Precision}
Precision measures the proportion of true positive predictions among all positive predictions:

\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\subsubsection{Recall (Sensitivity)}
Recall measures the proportion of true positive predictions among all actual positives:

\begin{equation}
\text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\subsubsection{F1-Score}
F1-Score is the harmonic mean of precision and recall, providing a balance between the two:

\begin{equation}
\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\subsubsection{Area Under the ROC Curve (AUC)}
AUC measures the model's ability to distinguish between classes and is robust to class imbalance:

\begin{equation}
\text{AUC} = \int_{0}^{1} TPR(FPR^{-1}(t)) dt
\end{equation}

where TPR is the true positive rate and FPR is the false positive rate.

\subsection{Hyperparameter Tuning}
To optimize model performance, we performed hyperparameter tuning using 5-fold cross-validation with grid search. The hyperparameters tuned for each algorithm were:

\begin{itemize}
    \item Logistic Regression: C (regularization strength), solver, penalty
    \item Random Forest: n\_estimators, max\_depth, min\_samples\_split, min\_samples\_leaf
    \item Gradient Boosting: n\_estimators, learning\_rate, max\_depth, min\_samples\_split
    \item SVM: C, gamma, kernel
    \item KNN: n\_neighbors, weights, p (distance metric)
    \item Neural Network: hidden\_layer\_sizes, activation, alpha, learning\_rate
\end{itemize}

\subsection{Feature Importance Analysis}
We conducted feature importance analysis to identify the most significant predictors of diabetes risk. For tree-based models (Random Forest and Gradient Boosting), we extracted feature importances directly from the trained models. For other algorithms, we used permutation importance, which measures the decrease in model performance when a feature is randomly shuffled.

\subsection{Web Interface Development}
To make our prediction model accessible for practical use, we developed a web-based interface using Streamlit, a Python library for creating interactive data applications. The interface allows users to input patient data and receive a diabetes risk prediction with probability and risk level (Low, Medium, High).

\section{Results and Discussion}
This section presents the experimental results and discusses the performance of the different machine learning algorithms for diabetes prediction.

\subsection{Model Performance Comparison}
Table I shows the performance metrics of the six ML algorithms on the test set after hyperparameter tuning.

\begin{table}[htbp]
\caption{Performance Comparison of Machine Learning Algorithms}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{AUC} \\
\hline
Logistic Regression & 0.7446 & 0.6719 & 0.5309 & 0.5931 & 0.8363 \\
\hline
Random Forest & 0.7489 & 0.6716 & 0.5556 & 0.6081 & 0.8175 \\
\hline
Gradient Boosting & 0.7446 & 0.6719 & 0.5309 & 0.5931 & 0.8335 \\
\hline
SVM & 0.7446 & 0.6833 & 0.5062 & 0.5816 & 0.8175 \\
\hline
KNN & 0.7446 & 0.6528 & 0.5802 & 0.6144 & 0.8034 \\
\hline
Neural Network & 0.7273 & 0.6216 & 0.5679 & 0.5935 & 0.7849 \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}

The K-Nearest Neighbors algorithm achieved the highest F1-score (0.6144) among all models, indicating a good balance between precision and recall. Random Forest had the highest accuracy (0.7489), while Logistic Regression showed the highest AUC (0.8363), suggesting better discrimination ability across different threshold settings.

After hyperparameter tuning, the optimized KNN model with 9 neighbors and uniform weights achieved an F1-score of 0.6040, which was slightly lower than the original model. This suggests that the initial configuration was already well-suited for the dataset.

\subsection{Confusion Matrix Analysis}
Fig. 1 shows the confusion matrix for the best-performing model (KNN) on the test set.

The confusion matrix reveals that the model correctly identified 114 out of 150 non-diabetic cases (76.0\%) and 47 out of 81 diabetic cases (58.0%). The relatively high false negative rate (34 out of 81 diabetic cases, or 42.0\%) is a concern in a medical context, as it represents missed diagnoses of diabetes. This suggests that while the model performs reasonably well overall, there is room for improvement in detecting positive cases.

\subsection{Feature Importance Analysis}
Fig. 2 illustrates the importance of each feature in the Random Forest model, which provides interpretable feature importance scores.

The feature importance analysis revealed that Glucose was the most significant predictor of diabetes, followed by BMI and Age. This aligns with clinical knowledge, as elevated blood glucose levels are directly related to diabetes, and higher BMI and age are known risk factors. The Diabetes Pedigree Function, which represents family history, was also an important predictor, consistent with the hereditary nature of type 2 diabetes.

\subsection{ROC and Precision-Recall Curves}
Fig. 3 shows the Receiver Operating Characteristic (ROC) curve for the KNN model, with an AUC of 0.8034.

The ROC curve demonstrates the model's ability to distinguish between diabetic and non-diabetic cases across different threshold settings. The AUC of 0.8034 indicates good discriminative power, with the model performing significantly better than random chance (AUC = 0.5).

Fig. 4 presents the Precision-Recall curve, which is particularly informative for imbalanced datasets like ours.

The Precision-Recall curve shows the trade-off between precision and recall at different threshold values. The area under this curve (PR-AUC) was 0.6723, indicating moderate performance in balancing precision and recall, which is challenging given the class imbalance in the dataset.

\subsection{Learning Curve Analysis}
Fig. 5 displays the learning curve for the KNN model, showing how performance changes with increasing training set size.

The learning curve shows that both training and cross-validation scores stabilize as the training set size increases, with a small gap between them. This suggests that the model is neither underfitting nor overfitting significantly. However, the plateau in performance indicates that adding more data of similar characteristics might not substantially improve the model's performance, and other approaches such as feature engineering or ensemble methods might be needed.

\subsection{Cross-Validation Results}
We performed 5-fold cross-validation to assess the model's stability and generalization ability. The KNN model achieved a mean cross-validation F1-score of 0.6135 with a standard deviation of 0.0655, indicating reasonable stability across different data subsets.

\subsection{Web Interface for Diabetes Prediction}
We developed a web-based interface using Streamlit to make our prediction model accessible for practical use. The interface includes:

\begin{itemize}
    \item Input fields for patient data (Pregnancies, Glucose, Blood Pressure, etc.)
    \item Prediction results with probability and risk level (Low, Medium, High)
    \item Recommendations based on the predicted risk level
    \item Visualizations of model performance and feature importance
\end{itemize}

The interface provides a user-friendly way for healthcare providers to use the prediction model in clinical settings, potentially aiding in early diabetes risk assessment and intervention.

\subsection{Discussion}
Our comparative analysis of six ML algorithms for diabetes prediction yielded several important findings:

\begin{itemize}
    \item KNN achieved the highest F1-score (0.6144), making it the best-performing algorithm overall when considering the balance between precision and recall.
    \item Logistic Regression showed the highest AUC (0.8363), indicating good discrimination ability across different threshold settings.
    \item Random Forest had the highest accuracy (0.7489) but slightly lower recall compared to KNN.
    \item Glucose level, BMI, and Age were identified as the most significant predictors of diabetes risk, aligning with clinical knowledge.
    \item The relatively high false negative rate (42.0\% of diabetic cases) suggests room for improvement in detecting positive cases.
\end{itemize}

These results are comparable to those reported in the literature. For instance, Perveen et al. \cite{perveen2016} achieved an accuracy of 81.97\% using AdaBoost with Random Forest, while Zou et al. \cite{zou2018} reported 83.8\% accuracy with SVM. Our models showed slightly lower accuracy (74.46-74.89\%), which could be attributed to our rigorous handling of missing values and the use of a separate test set for evaluation, providing a more realistic assessment of model performance.

The performance of our models is also influenced by the characteristics of the Pima Indians Diabetes Dataset, which has several limitations:

\begin{itemize}
    \item Limited sample size (768 instances)
    \item Class imbalance (34.9\% diabetic, 65.1\% non-diabetic)
    \item High proportion of missing values in some features (e.g., 48.70\% in Insulin)
    \item Restricted to female patients of Pima Indian heritage, limiting generalizability
\end{itemize}

Despite these limitations, our study demonstrates the potential of ML algorithms for diabetes risk prediction and provides insights for developing more accurate and practical prediction models.

\section{Conclusion and Future Work}
This paper presented a comprehensive comparative analysis of six machine learning algorithms for diabetes risk prediction using the Pima Indians Diabetes Dataset. We implemented a systematic preprocessing approach, evaluated model performance using multiple metrics, conducted feature importance analysis, and developed a web-based interface for practical application.

Our findings indicate that the K-Nearest Neighbors algorithm achieved the best overall performance with an F1-score of 0.6144 and accuracy of 74.46\% on the test set. Feature importance analysis revealed that glucose level, BMI, and age were the most significant predictors of diabetes risk, aligning with clinical knowledge.

The results of this study contribute to the growing body of research on machine learning applications in healthcare and provide insights for developing more accurate diabetes risk assessment tools. The web-based interface we developed demonstrates the potential for practical implementation of ML models in clinical settings, potentially aiding in early diabetes risk assessment and intervention.

Future work could focus on several directions:

\begin{itemize}
    \item Exploring advanced ensemble methods and deep learning approaches to improve prediction accuracy
    \item Incorporating additional features such as lifestyle factors, dietary habits, and genetic markers
    \item Addressing the class imbalance issue using techniques such as SMOTE (Synthetic Minority Over-sampling Technique)
    \item Validating the models on larger and more diverse datasets to enhance generalizability
    \item Conducting prospective studies to evaluate the clinical impact of the prediction models in real-world settings
\end{itemize}

By advancing research in these areas, we can develop more accurate and practical tools for diabetes risk prediction, contributing to early detection and prevention of this increasingly prevalent chronic disease.

\begin{thebibliography}{00}
\bibitem{idf2021} International Diabetes Federation, "IDF Diabetes Atlas, 10th edition," 2021. [Online]. Available: https://diabetesatlas.org/

\bibitem{maniruzzaman2017} M. Maniruzzaman et al., "Comparative approaches for classification of diabetes mellitus data: Machine learning paradigm," Computer Methods and Programs in Biomedicine, vol. 152, pp. 23-34, 2017.

\bibitem{zou2018} Q. Zou, K. Qu, Y. Luo, D. Yin, Y. Ju, and H. Tang, "Predicting Diabetes Mellitus With Machine Learning Techniques," Frontiers in Genetics, vol. 9, p. 515, 2018.

\bibitem{almahdawi2023} A. Almahdawi, Z. S. Naama, and A. Al-Taie, "Diabetes Prediction Using Machine Learning," 2022 3rd Information Technology to Enhance e-learning and Other Application (IT-ELA), 2023.

\bibitem{perveen2016} S. Perveen, M. Shahbaz, A. Guergachi, and K. Keshavjee, "Performance Analysis of Data Mining Classification Techniques to Predict Diabetes," Procedia Computer Science, vol. 82, pp. 115-121, 2016.

\bibitem{sisodia2018} D. Sisodia and D. S. Sisodia, "Prediction of Diabetes using Classification Algorithms," Procedia Computer Science, vol. 132, pp. 1578-1585, 2018.

\bibitem{uddin2023} A. M. Uddin and A. Ali, "Diabetes Prediction With Machine Learning Techniques," 2023 Global Conference on Information Technology and Computer Science (GCITCS), 2023.

\bibitem{kavakiotis2017} I. Kavakiotis, O. Tsave, A. Salifoglou, N. Maglaveras, I. Vlahavas, and I. Chouvarda, "Machine Learning and Data Mining Methods in Diabetes Research," Computational and Structural Biotechnology Journal, vol. 15, pp. 104-116, 2017.

\bibitem{lai2019} H. Lai, H. Huang, K. Keshavjee, A. Guergachi, and X. Gao, "Predictive models for diabetes mellitus using machine learning techniques," BMC Endocrine Disorders, vol. 19, no. 1, p. 101, 2019.

\bibitem{sneha2019} N. Sneha and T. Gangil, "Analysis of diabetes mellitus for early prediction using optimal features selection," Journal of Big Data, vol. 6, no. 1, p. 13, 2019.

\bibitem{naz2023} S. Naz and S. Bhatia, "Diabetes Prediction System Using Machine Learning with Web App," 2023 International Conference on Artificial Intelligence and Smart Systems (ICAIS), 2023.

\bibitem{islam2018} M. K. Islam, A. Siddique, M. S. I. Khan, A. K. M. N. Islam, M. N. Uddin, and M. R. Hossain, "Prediction of Diabetes Using Machine Learning Algorithms in Healthcare," 2018 24th International Conference on Pattern Recognition (ICPR), 2018.

\bibitem{sarwar2024} A. Sarwar, O. Sharma, M. Amir, and F. Ullah, "Diabetes Prediction Using Enhanced and Optimized DRL-CNN Algorithm," IEEE Access, vol. 12, pp. 58926-58940, 2024.

\bibitem{mir2018} A. Mir and S. N. Dhage, "Diabetes Disease Prediction Using Machine Learning on Big Data of Healthcare," 2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA), 2018.

\bibitem{kumari2013} V. Anuja Kumari and R. Chitra, "Classification Of Diabetes Disease Using Support Vector Machine," International Journal of Engineering Research and Applications, vol. 3, no. 2, pp. 1797-1801, 2013.

\bibitem{tigga2020} N. P. Tigga and S. Garg, "Prediction of Type 2 Diabetes using Machine Learning Classification Methods," Procedia Computer Science, vol. 167, pp. 706-716, 2020.

\bibitem{alic2017} B. Alić, L. Gurbeta, and A. Badnjević, "Machine learning techniques for classification of diabetes and cardiovascular diseases," 2017 6th Mediterranean Conference on Embedded Computing (MECO), 2017.

\bibitem{mujumdar2019} A. Mujumdar and V. Vaidehi, "Diabetes Prediction using Machine Learning Algorithms," Procedia Computer Science, vol. 165, pp. 292-299, 2019.

\bibitem{negi2016} A. Negi and V. Jaiswal, "A first attempt to develop a diabetes prediction method based on different global datasets," 2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC), 2016.

\bibitem{olivera2017} A. R. Olivera et al., "Comparison of machine-learning algorithms to build a predictive model for detecting undiagnosed diabetes - ELSA-Brasil: accuracy study," Sao Paulo Medical Journal, vol. 135, no. 3, pp. 234-246, 2017.

\end{thebibliography}

\end{document}
